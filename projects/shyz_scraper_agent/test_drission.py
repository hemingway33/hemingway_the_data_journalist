#!/usr/bin/env python3
"""
æµ‹è¯•DrissionPageç‰ˆæœ¬çš„çˆ¬è™«
"""

import logging
from shyz_scraper_drission import SHYZScraperDrission, CompanyQuery

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_drission_scraper():
    """æµ‹è¯•DrissionPageç‰ˆæœ¬çš„çˆ¬è™«"""
    print("=== æµ‹è¯•DrissionPageç‰ˆæœ¬çˆ¬è™« ===")
    print("æµ‹è¯•æŸ¥è¯¢: ä¸Šæµ·é‡åœ†å®ä¸šæœ‰é™å…¬å¸")
    print("ä¿¡ç”¨ä»£ç : 91310000MAE97Y0K5Y")
    print("ä½¿ç”¨æ¥å£ç›‘å¬æ–¹å¼è·å–ç»“æœ")
    print()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = SHYZScraperDrission(
        headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨ä¾¿äºè§‚å¯Ÿ
        timeout=30
    )
    
    try:
        print("1. å¯¼èˆªåˆ°æŸ¥è¯¢é¡µé¢...")
        success = scraper.navigate_to_query_page()
        
        if success:
            print("âœ… æˆåŠŸè¿›å…¥æŸ¥è¯¢é¡µé¢")
            
            print("2. æ‰§è¡ŒæŸ¥è¯¢å¹¶ç›‘å¬ç½‘ç»œè¯·æ±‚...")
            query = CompanyQuery(
                company_name="ä¸Šæµ·é‡åœ†å®ä¸šæœ‰é™å…¬å¸",
                credit_code="91310000MAE97Y0K5Y"
            )
            
            results = scraper.search_company_seal(query)
            
            if results:
                print(f"âœ… æŸ¥è¯¢æˆåŠŸ! æ‰¾åˆ° {len(results)} æ¡å°ç« è®°å½•:")
                for i, seal in enumerate(results, 1):
                    print(f"  {i}. å°ç« åç§°: {seal.seal_name}")
                    print(f"     å¤‡æ¡ˆæ—¥æœŸ: {seal.registration_date}")
                    print(f"     çŠ¶æ€: {seal.seal_status}")
                    print()
                
                # ä¿å­˜ç»“æœ
                json_file = scraper.save_results(results, "json", "test_drission_results")
                csv_file = scraper.save_results(results, "csv", "test_drission_results")
                print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {json_file} å’Œ {csv_file}")
                
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å°ç« è®°å½•")
                print("å¯èƒ½åŸå› :")
                print("   - è¯¥å…¬å¸ç¡®å®æ²¡æœ‰ç™»è®°å°ç« ")
                print("   - ç½‘ç«™æ¥å£å‘ç”Ÿå˜åŒ–")
                print("   - ç½‘ç»œè¯·æ±‚ç›‘å¬æœªæ•è·åˆ°ç›¸å…³æ•°æ®")
                
                # æ‰“å°æ•è·åˆ°çš„ç½‘ç»œè¯·æ±‚ä¿¡æ¯
                if scraper.network_requests:
                    print(f"\nğŸ“Š æ•è·åˆ° {len(scraper.network_requests)} ä¸ªç½‘ç»œè¯·æ±‚:")
                    for i, req in enumerate(scraper.network_requests, 1):
                        print(f"  {i}. {req['method']} {req['url']}")
                        print(f"     æ—¶é—´: {req['timestamp']}")
                else:
                    print("\nâš ï¸ æœªæ•è·åˆ°ç›¸å…³çš„ç½‘ç»œè¯·æ±‚")
        else:
            print("âŒ æ— æ³•è¿›å…¥æŸ¥è¯¢é¡µé¢")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        
    finally:
        print("\n3. å…³é—­æµè§ˆå™¨...")
        scraper.close()
        print("æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_drission_scraper() 